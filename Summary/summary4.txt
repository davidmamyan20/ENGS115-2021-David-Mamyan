01.10.21

In today's lecture we started discussing different ways of sorting, which we were required to watch videos about it. We start with the selection sort. It chooses the current minimum, and compares with the current element of the array, if it's bigger than the minimum, their positions won't be swapped, if it's smaller than the minimum, their positions will swap. We use the first element for comparison, and when it is sorted, next time the loop works it will ignore the first element, and go to the next one. And after the first comparision current position and i, which is also a position of an array ++. About the bubble code which compare A[i] and A[i+1], and it will swap if A[i]>A[i+1], and the loop will be repeated again, unless the array is sorted. And every time it runs the last element is ignored, every time. We find the complexity of the bubble sort by adding numbers from 1 to n which is equal to n(n-1)/2, and after droping the constants and n with lower powers, we get O(n^2).

04.10.21

Today, we discussed the complexity of the selection sort, which is equal to n^2, because mathematically we know that the sum of the numbers from 1 to n is equal to n(n-1)/2, which is equal to (n^2-n)/2, so we drop the constants and the n with higher degree, and get O(n^2). Then we discussed the bubble sort, and compared it with the selection sort, in the bubble sort we compare the pairs of element, whereas in the selection sort we compare the current minimum, with the rest of the elements in the array. The insertion sort starts with the left first element, assuming it is in the right position, then compares it with the elements on the right, and if they are smaller, they swap places, in order to determine the correct position of the elements. The complexity of the bubble sort is also (n^2). The complexity of the insertion sort is O(n^2) in the worst case, when the elements are sorted in the reverse order, in other words there will be swaps, after every comparison. We simply calculate it by adding the numbers from 1 to n-1, and get O(n^2). It is not good, when the complexity is O(n^2), because when elements increase by 1, the complexity grows at a faster rate. Merge sort is about the basic of divide and conquer. WE can guess that the complexity is logarithmic because of divide and conquer. After dividing, the part of sticking them together is tricky, in case of two element arrays, it is not hard, but when it comes to making an array of four elements, is tough, we have to find all minimums to fix the position of the element, and the same goes for the rest. This sort is a recursive algorithm. We find the complexity by finding the quanity of the steps of dividing and conquering. In the first step we get n/2 in the second step n/4, so we have to do until n/2^x, in other words there are x steps in this algorithm. n/2^x = 1 x = log2n. The complexity of the divisions in log2n. In the end we know that the complexity is nlogn.  That's all for this class.

06.10.21

Today, we are implementing a selection sort algorithm in an array. First, we need to select the largest item. Secondly, we should swap the largest item with the last item, moving it to the end. Then we ignore the largest last item, then search the rest of the array for the largest item. As we start from 0, our last element is n-1, not n. Then we do an array swap with n-2, because n-1 is fixed. In case if the indexes n and i are the same, then the program will not do the required action, so we need an if(l != i), then swap, in order to make the program work. We must swap the largest item with the last item (next-to-last in the original array), then continue until n-1 items are selected and swapped (the remaining item on the first index). The start of the array is the pointer to the first element of the array. Then we do insertion sort. For the first step we set the Array[0] as the sorted part of the array and Array[1] - Array [n - 1] as not sorted part of the array, then take the first item from the not sorted array and find its proper place in the sorted part.
